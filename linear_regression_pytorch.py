# -*- coding: utf-8 -*-
"""Copy of linear_regression_pytorch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cDdBw2CNNbPONWd_6sOsMjgOxL7_45CR
"""

import torch
import numpy as np
x = np.array([[1,2],[3,4.]])
x
y = torch.tensor(x)
y
print(x.dtype, y.dtype)

t3 = torch.tensor([
    [[11,12,13,14],
     [13,14,15,16]],
    [[15,16,17,18],
     [16,17,18,19.]],
    [[15,16,17,18],
     [16,17,18,19.]]
])
t3
t3.shape

x = torch.tensor(3.)
w = torch.tensor(4., requires_grad=True)
b = torch.tensor(5., requires_grad=True)
y = w * x + b
y

y.backward()
print('dy/dx', x.grad)
print('dy/dw', w.grad)
print('dy/db', b.grad)



import numpy as np
import torch
import math
#region, temp,rain,humidity,apples,oranges
#kanto, 73, 67, 43, 56, 70
#johto, 91, 88, 64 ,81, 101
#hoen, 87, 134, 58, 119, 133
#sinnoh, 102, 43,37, 22, 37
#unova, 69, 96, 70, 103, 119
#yield_apple = w11 * temp + w12 * rainfall + w13 * humidity + b1
#yield_orange = w21 * temp + w22 * rainfall + w23 * humidity + b2

inputs = np.array([
[73,67,43],
[91,88,64],
[87,134,58],
[102,43,37],
[69,96,70]], dtype='float32')
targets = np.array([
[56,70],
[81,101],
[119,133],
[22,37],
[103,119]], dtype='float32')
#avg 76.2, 92
inputs = torch.tensor(inputs)
targets = torch.from_numpy(targets)
print(inputs.shape)
print(targets.shape)
print(inputs)
print(targets)

"""torch.randn creates a tensor with the given shape, with elements pickedrandomly from a normal distribution with mean 0 and stand deviation 1.
Model is a function that performs a matrix multiplication of the inputs and the weights w (transposed) and adds the bias b (replicated for each observation)
"""

w = torch.randn(2,3,requires_grad=True)
b = torch.randn(2,requires_grad=True)
print(w)
print(b)
def model(x):
    return x @ w.t() + b

"""@ represents matrix mulitiplication, and .t metod returns transpose of a tensor.
matrix obtained by passing the input data into the model is a set of predictions for the target vars.
"""

preds = model(inputs)
print(preds)

diff = preds - targets
print(diff)
diff_sqr = diff * diff
print(diff_sqr)
manual_mse = torch.sum(diff_sqr) / diff.numel()
print(manual_mse)
print(math.sqrt(manual_mse))

def mse(t1,t2):
  diff = t1 - t2
  return torch.sum(diff * diff) / diff.numel()
loss = mse(preds, targets)
print(loss)

""".nume1 method returns the nummber of elements in a tensor
Loss is quadratic function of weights and bieases

After calling backward(), the gradients are stored in the .grad attribute of the tensors.

gradients are stored in the .grad property of the respective tensors. Note derivative of the loss w.r.t. the weights matrix is itself a matrix, with same dimentsion
"""

preds = model(inputs)
loss = mse(preds, targets)
loss.backward()
print(w)
print(b)
print(w.grad)
print(b.grad)
print(loss)

with torch.no_grad():
  w -= w.grad * 1e-5
  b -= b.grad * 1e-5
  w.grad.zero_()
  b.grad.zero_()
print(w)
print(b)

preds = model(inputs)
loss = mse(preds, targets)
print(loss)

"""Significant Reduction of Loss!!!"""

for i in range(1000):
  preds = model(inputs)
  loss = mse(preds, targets)
  loss.backward()
  with torch.no_grad():
    w -= w.grad * 1e-5
    b -= b.grad * 1e-5
    w.grad.zero_()
    b.grad.zero_()

preds = model(inputs)
loss = mse(preds, targets)
print(loss)
print(math.sqrt(loss))
print(b)
print(w)

"""========================================================"""

import torch.nn as nn
from torch.utils.data import TensorDataset
from torch.utils.data import DataLoader
import torch.nn.functional as F
inputs = np.array([
[73,67,43],
[91,88,64],
[87,134,58],
[73,67,43],
[91,88,64],
[87,134,58],
[102,43,37],
[69,96,70],
[102,43,37],
[69,96,70],
[73,67,43],
[91,88,64],
[87,134,58],
[102,43,37],
[69,96,70]], dtype='float32')

targets = np.array([
[56,70],
[81,101],
[119,133],
[56,70],
[81,101],
[119,133],
[22,37],
[103,119],
[22,37],
[103,119],
[56,70],
[81,101],
[119,133],
[22,37],
[103,119]], dtype='float32')
#avg 76.2, 92
inputs = torch.tensor(inputs)
targets = torch.from_numpy(targets)
print(inputs.shape)
print(targets.shape)
print(inputs)
print(targets)

#define dataset
train_ds = TensorDataset(inputs, targets)
train_ds[0:3]

"""TDset allows access to small section of training data using array [0:3]. returning a typle pair in which 1st element contains input variables for selected rows and 2nd, the target"""

batch_size = 5
#manual
train_dl = DataLoader(train_ds, batch_size, shuffle=True)
for xb, yb in train_dl:
  print(xb)
  print(yb)
  break

model = nn.Linear(3,2)
print(model.weight)
print(model.bias)

preds = model(inputs)
print(preds)

loss_fn = F.mse_loss
loss = loss_fn(model(inputs), targets)
print(loss)

opt = torch.optim.SGD(model.parameters(), lr=1e-5)

"""1. Generate predictions
2. Calculate the loss
3. Compute gradients with respect to Weights and Biases
4. Adjust the weights by subtracting a small quantity proportional to the gradient
5. Reset gradients to zero
"""

def fit(num_epochs, model, loss_fn, opt):
  for epoch in range(num_epochs):
    for xb, yb in train_dl:
      pred = model(xb)
      loss = loss_fn(pred, yb)
      loss.backward()
      opt.step()
      opt.zero_grad()
    if (epoch+1) % 10 == 0:
      print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))

"""Stochastic Gradient Descent (stochastic due to batches instead of single group)

.parameters method returns list containing all weights and bias matrices
"""

fit(100, model, loss_fn, opt)

preds = model(inputs)
preds

targets